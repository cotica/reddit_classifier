{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping subreddits\n",
    "\n",
    "\n",
    "## Part II - EDA\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import all our required libraries up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T23:14:59.933337Z",
     "start_time": "2021-04-27T23:14:36.666361Z"
    }
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd, numpy as np, requests, time, nltk, datetime as dt\n",
    "\n",
    "#NLP\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import gensim.downloader as api #allows us to get word2vec anf glove embeddins that we need\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "#classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, plot_confusion_matrix, \\\n",
    "                             recall_score, precision_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "# easier to see full text with a bigger maxwidth:\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T07:58:26.010144Z",
     "start_time": "2021-04-25T07:58:26.005832Z"
    }
   },
   "outputs": [],
   "source": [
    "#pip install python-Levenshtein\n",
    "#this is for the gensim.similarities.levenshtein submodule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull in the dataframes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T22:48:23.094034Z",
     "start_time": "2021-04-25T22:48:22.691474Z"
    }
   },
   "outputs": [],
   "source": [
    "movie_deets2 = pd.read_csv('../data_outputs/good_movie_deets2.csv', index=False)\n",
    "bad_deets2 = pd.read_csv('../data_outputs/bad_movie_deets2.csv', index=False)\n",
    "all_movies = pd.read_csv('../data_outputs/all_movies.csv', index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at basic stats about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T19:35:56.841690Z",
     "start_time": "2021-04-25T19:35:56.833697Z"
    }
   },
   "outputs": [],
   "source": [
    "#<df>.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand the density and usefulness of our content, by analyzing the volume of comments and looking for the richness of the text-heavy columns we are expecting.\n",
    "\n",
    "We are also going to see, whether there are any interesting prospective features for training our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T19:35:56.850957Z",
     "start_time": "2021-04-25T19:35:56.847503Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#<df>['num_comments'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T19:35:56.912051Z",
     "start_time": "2021-04-25T19:35:56.890906Z"
    }
   },
   "outputs": [],
   "source": [
    "#check for post density\n",
    "#<df>['selftext'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T19:35:56.919058Z",
     "start_time": "2021-04-25T19:35:56.914422Z"
    }
   },
   "outputs": [],
   "source": [
    "#<df>['title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T19:35:56.930409Z",
     "start_time": "2021-04-25T19:35:56.925501Z"
    }
   },
   "outputs": [],
   "source": [
    "#<df>['author'].value_counts() #get only unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP & feature eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T19:35:56.940228Z",
     "start_time": "2021-04-25T19:35:56.935640Z"
    }
   },
   "outputs": [],
   "source": [
    "#word freq\n",
    "#vectorize self-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T19:35:56.947726Z",
     "start_time": "2021-04-25T19:35:56.943275Z"
    }
   },
   "outputs": [],
   "source": [
    "#sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
